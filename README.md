# DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving
> ğŸ“œ [[Arxiv](http://arxiv.org/abs/2510.12796)] ğŸ¤— [[Model Weights](https://huggingface.co/liyingyan/DriveVLA-W0)]

Yingyan Li*, Shuyao Shang*, Weisong Liu*, Bing Zhan*, Haochen Wang*, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fanâ€ , Zhaoxiang Zhangâ€ 

This Paper presents **DriveVLA-W0**, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment, remedying the "supervision deficit" in VLA models and amplifying data scaling laws.

<p align="center">
  <img src="assets/fig1.png" alt="DriveVLA-W0" width="1000"/>
</p>


> Due to company policy, only the reviewed portion of our code is currently available. Please contact us if you have any questions.

## ğŸ“‹ é¡¹ç›®ç»“æ„

```
DriveVLA-W0/
â”œâ”€â”€ assets/                    # é¡¹ç›®èµ„æºæ–‡ä»¶ï¼ˆå›¾ç‰‡ã€æ–‡æ¡£ç­‰ï¼‰
â”œâ”€â”€ configs/                   # æ¨¡å‹é…ç½®æ–‡ä»¶å’Œå½’ä¸€åŒ–ç»Ÿè®¡
â”‚   â”œâ”€â”€ fast/                 # fast action tokenizer
â”‚   â”œâ”€â”€ normalizer_navsim_test/    # NAVSIMæµ‹è¯•æ•°æ®å½’ä¸€åŒ–é…ç½®
â”‚   â”œâ”€â”€ normalizer_navsim_trainval/ # NAVSIMè®­ç»ƒéªŒè¯æ•°æ®å½’ä¸€åŒ–é…ç½®
â”‚   â””â”€â”€ normalizer_nuplan/    # NuPlanæ•°æ®é›†å½’ä¸€åŒ–é…ç½®
â”œâ”€â”€ data/                      # æ•°æ®å¤„ç†å’Œé…ç½®
â”‚   â”œâ”€â”€ navsim/               # NAVSIMæ•°æ®é›†ç›¸å…³
â”‚   â””â”€â”€ others/               # å…¶ä»–æ•°æ®é›†
â”œâ”€â”€ inference/                 # æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ navsim/               # NAVSIM PDMSè¯„æµ‹
â”‚   â”œâ”€â”€ qwen/                 # Qwenæ¨¡å‹æ¨ç†
â”‚   â””â”€â”€ vla/                  # Emuæ¨¡å‹æ¨ç†
â”œâ”€â”€ models/                    # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ policy_head/          # ç­–ç•¥å¤´å®ç°
â”‚   â””â”€â”€ tokenizer/            # åˆ†è¯å™¨å®ç°
â”œâ”€â”€ scripts/                   # è®­ç»ƒå’Œéƒ¨ç½²è„šæœ¬
â”œâ”€â”€ tools/                     # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ action_tokenizer/     # åŠ¨ä½œåˆ†è¯å™¨
â”‚   â””â”€â”€ pickle_gen/           # æ•°æ®é¢„å¤„ç†å’Œpickleç”Ÿæˆ
â”œâ”€â”€ train/                     # è®­ç»ƒä»£ç 
â”‚   â”œâ”€â”€ datasets.py           # æ•°æ®é›†å®šä¹‰
â”‚   â”œâ”€â”€ train_ar.py           # è‡ªå›å½’æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ train_moe.py          # MoEæ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ train_pi0.py          # PI0æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ train_qformer.py      # QFormeræ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ train_qwen_vla.py     # Qwen-VLAè”åˆè®­ç»ƒ
â”‚   â””â”€â”€ train.py              # ä¸»è®­ç»ƒè„šæœ¬
â””â”€â”€ requirements.txt          # Pythonä¾èµ–
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 5åˆ†é’Ÿä¸Šæ‰‹ç¤ºä¾‹

1. **ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹**
```bash
pip install huggingface_hub
export HF_ENDPOINT=https://hf-mirror.com
mkdir pretrained_models
bash scripts/misc/download.sh
```

2. **ç¯å¢ƒè®¾ç½®**
```bash
conda create -n drivevla python=3.10
conda activate drivevla
pip install -r requirements.txt
```

3. **ä¸‹è½½æ¨¡å‹æƒé‡**
```bash
# ä»Hugging Faceä¸‹è½½é¢„è®­ç»ƒæƒé‡
# æƒé‡å°†ä¿å­˜åœ¨ pretrained_models/ ç›®å½•ä¸‹
```

4. **è¿è¡Œæ¨ç†**
```bash
# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†
bash inference/vla/infer_navsim_flow_matching_PDMS_87.2.sh
```

### å®Œæ•´è®­ç»ƒæµç¨‹

å¦‚æœæ‚¨æƒ³ä»å¤´è®­ç»ƒæ¨¡å‹ï¼Œè¯·å‚è€ƒ [Training.md](Training.md) è·å–è¯¦ç»†çš„è®­ç»ƒæŒ‡å—ã€‚

## ğŸ“Š æ•°æ®å‡†å¤‡

### NAVSIMæ•°æ®é›†

DriveVLA-W0 ä½¿ç”¨ NAVSIM (v1.1) æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚æ‚¨éœ€è¦ï¼š

1. **è·å–NAVSIMæ•°æ®é›†**
   - è®¿é—® [NAVSIMå®˜æ–¹ä»“åº“](https://github.com/autonomousvision/navsim/tree/v1.1)
   - ä¸‹è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
   - æ•°æ®åŒ…å«ä¼ æ„Ÿå™¨æ•°æ®ã€åœºæ™¯ä¿¡æ¯å’Œæ ‡æ³¨

2. **æ•°æ®é¢„å¤„ç†**
   ```bash
   # ç”ŸæˆVQç´¢å¼•
   python tools/pickle_gen/pickle_generation_navsim_pre_1s.py

   # ç”ŸæˆNAVSIM pickleæ–‡ä»¶
   bash scripts/tokenizer/extract_vq_emu3_navsim.sh
   ```

3. **æ•°æ®æ ¼å¼**
   - é¢„å¤„ç†åçš„æ•°æ®ä¿å­˜åœ¨ `data/navsim/processed_data/`
   - åŒ…å«åœºæ™¯æ–‡ä»¶ã€å…ƒæ•°æ®å’Œé¢„å¤„ç†åçš„ç‰¹å¾

### æ•°æ®é‡çº§
- **è®­ç»ƒæ•°æ®**: ~100Kå¸§é©¾é©¶åœºæ™¯
- **éªŒè¯æ•°æ®**: ~10Kå¸§
- **æµ‹è¯•æ•°æ®**: NAVSIMæµ‹è¯•é›†

## ğŸ’» ç¡¬ä»¶è¦æ±‚

### è®­ç»ƒèµ„æºæ¶ˆè€—
8x L20 GPU (40GB memory), ~16å°æ—¶


# Install



## CUDA install

å¦‚æœæ‚¨çš„ç³»ç»Ÿæ²¡æœ‰CUDA 12.4+ï¼Œè¯·å…ˆå®‰è£…ï¼š

```bash
# ä¸‹è½½CUDA 12.8.1 (æ¨èç‰ˆæœ¬)
wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda_12.8.1_570.124.06_linux.run

# å®‰è£…CUDAå·¥å…·åŒ…
bash cuda_12.8.1_570.124.06_linux.run --silent --toolkit --toolkitpath=/usr/local/cuda-12.8

# è®¾ç½®ç¯å¢ƒå˜é‡ (æ·»åŠ åˆ° ~/.bashrc)
export CUDA_HOME=/usr/local/cuda-12.8
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

## Conda ç¯å¢ƒè®¾ç½®

```bash
# åˆ›å»ºCondaç¯å¢ƒ
conda create -n drivevla python=3.10
conda activate drivevla

# å®‰è£…PyTorch (CUDA 12.4)
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install -r requirements.txt
pip install "transformers[torch]"

# å®‰è£…è®­ç»ƒç›¸å…³ä¾èµ–
pip install deepspeed          # åˆ†å¸ƒå¼è®­ç»ƒ
pip install scipy              # ç§‘å­¦è®¡ç®—
pip install tensorboard==2.14.0  # å¯è§†åŒ–
pip install wandb              # å®éªŒè·Ÿè¸ª
```


### Testing

First, please download the checkpoints from [Hugging Face](https://huggingface.co/liyingyan/DriveVLA-W0). 

Then, run the corresponding testing script to get output actions as json files
```
bash inference/vla/infer_navsim_with_previous_action_last_vava.sh
```
Finally, run the following script to compute PDMS from json files (using the conda enviroment with [navsim](https://github.com/autonomousvision/navsim/tree/v1.1))
```
bash inference/vla/run_emu_vla_navsim_metric_others.sh
```

## âš™ï¸ é…ç½®è¯´æ˜

### é…ç½®æ–‡ä»¶ç»“æ„

é¡¹ç›®ä½¿ç”¨JSONæ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼Œä½äº `configs/` ç›®å½•ï¼š

```
configs/
â”œâ”€â”€ moe_fast_video.json          # MoEæ¨¡å‹å¿«é€Ÿæ¨ç†é…ç½®
â”œâ”€â”€ moe_fast_video_pretrain.json # MoEæ¨¡å‹é¢„è®­ç»ƒé…ç½®
â”œâ”€â”€ normalizer_navsim_test/      # NAVSIMæµ‹è¯•æ•°æ®å½’ä¸€åŒ–å‚æ•°
â”œâ”€â”€ normalizer_navsim_trainval/  # NAVSIMè®­ç»ƒæ•°æ®å½’ä¸€åŒ–å‚æ•°
â””â”€â”€ normalizer_nuplan/           # NuPlanæ•°æ®å½’ä¸€åŒ–å‚æ•°
```

### å½’ä¸€åŒ–ç»Ÿè®¡

æ•°æ®å½’ä¸€åŒ–å‚æ•°æ ¹æ®è®­ç»ƒæ•°æ®é›†è‡ªåŠ¨è®¡ç®—ï¼š

- `normalizer_navsim_trainval/` - åŸºäºNAVSIMè®­ç»ƒé›†
- `normalizer_navsim_test/` - åŸºäºNAVSIMæµ‹è¯•é›†
- `normalizer_nuplan/` - åŸºäºNuPlanæ•°æ®é›†

# ğŸ† NAVSIM v1/v2 Benchmark SOTA

Here is a comparison with state-of-the-art methods on the NAVSIM test set, as presented in the paper. Our model, **DriveVLA-W0**, establishes a new state-of-the-art.

| Method | Reference | Sensors | NC â†‘ | DAC â†‘ | TTC â†‘ | C. â†‘ | EP â†‘ | PDMS â†‘ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Human** | | | 100.0 | 100.0 | 100.0 | 99.9 | 87.5 | 94.8 |
| **_BEV-based Methods_** | | | | | | | | |
| LAW | ICLR'25 | 1x Cam | 96.4 | 95.4 | 88.7 | 99.9 | 81.7 | 84.6 |
| Hydra-MDP | arXiv'24 | 3x Cam + L | 98.3 | 96.0 | 94.6 | 100.0 | 78.7 | 86.5 |
| DiffusionDrive | CVPR'25 | 3x Cam + L | 98.2 | 96.2 | 94.7 | 100.0 | 82.2 | 88.1 |
| WoTE | ICCV'25 | 3x Cam + L | 98.5 | 96.8 | 94.4 | 99.9 | 81.9 | 88.3 |
| **_VLA-based Methods_** | | | | | | | | |
| AutoVLA | NeurIPS'25 | 3x Cam | 98.4 | 95.6 | 98.0 | 99.9 | 81.9 | 89.1 |
| ReCogDrive | arXiv'25 | 3x Cam | 98.2 | 97.8 | 95.2 | 99.8 | 83.5 | 89.6 |
| **DriveVLA-W0*** | **Ours** | **1x Cam** | **98.7** | **99.1** | **95.3** | **99.3** | **83.3** | **90.2** |
| AutoVLAâ€  | NeurIPS'25 | 3x Cam | 99.1 | 97.1 | 97.1 | 100.0 | 87.6 | 92.1 |
| **DriveVLA-W0â€ ** | **Ours** | **1x Cam** | **99.3** | **97.4** | **97.0** | **99.9** | **88.3** | **93.0** |

# â­ Star 
If you find our work useful for your research, please consider giving this repository a star â­.

# ğŸ“œ Citation
If you find this work useful for your research, please consider citing our paper:
```
@article{li2025drivevla,
  title={DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving},
  author={Li, Yingyan and Shang, Shuyao and Liu, Weisong and Zhan, Bing and Wang, Haochen and Wang, Yuqi and Chen, Yuntao and Wang, Xiaoman and An, Yasong and Tang, Chufeng and others},
  journal={arXiv preprint arXiv:2510.12796},
  year={2025}
}
```

# Acknowledgements
We would like to acknowledge the following related works:

[**LAW (ICLR 2025)**](https://github.com/BraveGroup/LAW): Using latent world models for self-supervised feature learning in end-to-end autonomous driving.

[**WoTE (ICCV 2025)**](https://github.com/liyingyanUCAS/WoTE): Using BEV world models for online trajectory evaluation in end-to-end autonomous driving.

[**UniVLA**](https://github.com/baaivision/UniVLA): World modeling in the broader field of robotics.
